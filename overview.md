# Adventures in Autograding

The term "autograding" is not ideal - it tends to conjure up images of a free lunch. There's no free lunch here. Real work must be done here.

What I'm describing here is a system that provides some significant benefits to both students and instructors.

## The Benefits

### The Benefits to Students

In no particular order of importance, students gain the following benefits from the system:

- exposure to GitHub (at a simple level)
- the procedure they need to follow is the same for any code-creation assessment (drill, lab quiz, assignment, coding exam)
- the Eclipse project they use is configured to provide immediate feedback on test and code analysis every time they save their work
- the results they see in Eclipse are identical to those that happen when they submit, so there are no surprises when it comes to marking

### The Benefits to Instructors

- the compilation, testing, and analysis of student code is all automated
- summary reports of all submissions are placed automatically in a convenient location
- the stringency of analysis tools can be altered for any assessment by creating different configuration files (which will be able to be reused in many situations)
- personalized feedback of student code can be made in-line using GitHub's excellent code review tools

## The Costs

### Create assessments that can be tested in an automated way

- if you want to automate marking, you have to create assessments that can be tested easily. This typically means moving to assessments whose functionality can be tested with an automated tool; the easiest tool to reach for here is JUnit.
- this can be a challenging step; although you _can_ test with console input/output in JUnit, it's not ideal and can require a lot of regex work, which is not the most pleasant thing in the world. But once you start gaining experience using testing tools, this step becomes significantly easier...and dare I say...fun?

### Write JUnit tests

- the current summarizer tool that I've made only recognizes reports generated by JUnit 5...but it's the standard these days and has many useful features that aren't available in JUnit 4, so one should be using 5 anyway.
- there are some libraries available that make testing console input/output straightforward, so that kind of testing is available via JUnit if you want it (but keep in mind my warning from the previous section about issues with testing console input/output)
- since tests are the primary form of feedback for students, they have to be well-written and their failure messages should be clear and unambiguous. Ideally, your tests should be self-documenting, as students in later programming courses should be urged to read the tests as a source of documentation!

### Create Settings Files for Tools

- each static analysis tool needs a settings file that contains the rules to be enforced by the tool. These tools tend to be a bit overboard for new programmers, so it's important to create a fairly relaxed set of rules. Setting up the rules "just so" can be a tad time-consuming, but fortunately, once done, the settings can be re-used for all iterations of a course...at least until the tool providers change the format of the settings files, or deprecates certain rules! :)
- it's important to choose tools that are available as **both** a command-line tool (for the build process) **and** Eclipse plugin (for everyday coding). If this isn't the case, then a student will be frustrated if their IDE says everything is fine, but then the build fails. I'm currently suggesting PMD (for "best practice" checks) and Checkstyle (for style checks) as they both fit the bill.

### Work with GitHub a lot

- since every aspect of the system involves GitHub _somehow_, you need to be somewhat comfortable with Git...but not to a huge degree (don't need to branch or rebase or do anything beyond a "comfortable beginner" level)
- you also need to create GitHub Organizations for every course and get them approved as educational organizations by GitHub for every course. This isn't overly onerous, but it does have to be done - and since you have to wait on someone from GitHub to approve your request, it shouldn't be done at the last minute!

### Convert Summary Reports to Marks

- this is where things get interesting: you need to figure out how to map your summary results to final grades. For a pass/fail assessment, this might be stunningly easy - if there are **any** errors in compilation, static analysis, or unit tests, you fail - otherwise, you pass.
- but what if it's not a pass/fail situation? How do you weight the results of the static analysis? How do you weight the tests? Are some worth more than others? If so, by how much?
- and the fun doesn't stop there: even after you've decided the weightings of the results, how do you parse the summary report to generate the marks? I'm currently leaving that up to the reader - the summary reports have a very specific format, so creating a little tool that maps a text file to actual results shouldn't be overly onerous - but it's not a walk in the park either.
  > NOTE: if you're loathe to take the plunge on this step, be aware that there are some resources out there already that automate this "assigning points to tests/tools" process. I've worked with [gradlegrader](https://github.com/cs125-illinois/gradlegrader) and (RiceChecks)[https://github.com/RiceComp215-Staff/RiceChecks]. They both require Gradle to work, which is both a bane and a boon. They also are quite configurable...but that's a double-edged sword as well, right? In any case, if you want to talk about these tools, let's do it.

## Potential Gotchas

Just so nobody thinks I'm a Pollyanna about this, I can see some potential issues.

- currently, GitHub Classroom use is free, as is making educational organizations. What if this changes?
- each organization currently (as of 2020-09) gets 3000 minutes of build time; if you get some students who go crazy with submitting multiple times, or if the quota goes down, this system goes kaput
- are there any FOIP issues with using GitHub to store student code? It's pretty grey and the MRU administration seems pretty unprepared to clearly answer this.
